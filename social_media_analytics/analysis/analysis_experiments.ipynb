{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#EB7101; font-family: arial; color: #ffffff; font-size: 200%; text-align: center; border-radius: 15px 15px;\">0 - Libraries</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Thore\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from sma_modules import analysis, scraper, transformer_pipe\n",
    "\n",
    "import altair as alt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#EB7101; font-family: arial; color: #ffffff; font-size: 200%; text-align: center; border-radius: 15px 15px;\">1 - Scenario</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We at Chegg strive to empower students in reaching their full potential. Our mission thus entails student support throughout school and beyond. This mission is brought to life by our e-learning platform, offering programs spanning academics, personal growth, and skill-building. Thus, we seek to guarantee the utmost educational value for learners' subscription fees. The value proposition, however, may be at risk, as indicated in reports (data visualization project) relating to future educational delivery. Findings suggest landscape dynamics, leaning towards dialogue-optimized large language models.\n",
    "\n",
    "While a competitive large language model remains out of reach, recently launched customizable versions of OpenAI's ChatGPT may pose viable alternatives. Tailored versions, such as GPTs, might be introduced to complement existing programs, adapting to shifting landscape dynamics. Nevertheless, any investment directed at exploring this resource must undergo thorough due diligence. A preliminary indicator of investment potential is obtained via market research. \n",
    "\n",
    "This project serves the purpose of market research, whose goal is to ascertain public perceptions of GPTs. Hence, the emphasis on the prevailing social media sentiment, raising awareness as an indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#EB7101; font-family: arial; color: #ffffff; font-size: 200%; text-align: center; border-radius: 15px 15px;\">2 - Data</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sourcing data for this project, social media platform X, formerly known as Twitter, was consulted. X, for its diverse and large population, conducive to sentiment analysis, given myriads of opinions on various topics. Further, research on sentiment in social media commonly relies on X, wherefore downstream methodologies lend themselves to tweets. \n",
    "\n",
    "With respect to X's API revisit, free-tier tweet accessibility suffered, necessitating web scraping to acquire sufficient data for the project. X's dynamic elements, spanning login masks, search boxes, and infinite scrolling, presume **_Selenium_**. **_Selenium_** conducts daily searches at the granularest time scale, as a consequence of the shift towards curated feeds over extended horizons. In doing so, adjacent days are broached, sacrificing execution time for greater granularity. As proactive strategies for navigating such searches, scroll depths and intervals were opted, mitigating rate-limiting. In an effort to forestall disruptions, search histories are cleared, otherwise dynamically prolonged, inhibiting scraping relevant elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"secret.env\")\n",
    "\n",
    "scrape = scraper(\n",
    "    os.getenv(\"user_identifier\"),\n",
    "    os.getenv(\"password\"),\n",
    "    \"GPTs\", \n",
    "    \"en\", \n",
    "    \"%d.%m.%Y\",\n",
    "    \"09.11.2023\",\n",
    "    \"20.12.2023\" \n",
    ")\n",
    "\n",
    "#scrape tweets\n",
    "scrape.scraping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search scope constitutes the research target, GPTs. Thus, tweets featuring GPTs as keyword are sourced, particularly those in English, aligning with our target audience. Tweets sourced range from GPTs release date (09.11.2023) up until the project's start date (20.12.2023).\n",
    "\n",
    "Author, publication date, and text are factored in when speaking of tweet sourcing. Authorship and date serve as attribution to the source, a practice crucial in establishing research credibility. While primary opinions are captured by original tweet's texts, granting insight onto the broader conversation. Thus the core, fueling underlying sentiment remains, with replies sacrificed for scraper execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape.tweets_df.to_csv(\"tweets_unprocessed.csv\",\n",
    "                        index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following those criteria, 13,368 instances were scraped and stored in a CSV file for reference. Stored as a checkpoint for eventual future research, extending beyond this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#EB7101; font-family: arial; color: #ffffff; font-size: 200%; text-align: center; border-radius: 15px 15px;\">3 - Processing</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts of the aforementioned checkpoint constitute ingredients for natural language processing. As concerns sentiment, text classification takes center stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = transformer_pipe(\"tweets_unprocessed.csv\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sen_clas: 100%|██████████| 13368/13368 [38:50<00:00,  5.74texts/s]\n"
     ]
    }
   ],
   "source": [
    "#predict sentiments\n",
    "nlp.sens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With regard to classification, transformers qualify for their state-of-the-art performance. Among those transformers, RoBERTa stood out for its superior performance. RoBERTa has been fine-tuned before, coming in a variety of potentially compatible versions. From these, __[\"twitter-roberta-base-sentiment-latest\"](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)__ of __[\"Cardiff NLP\"](https://huggingface.co/cardiffnlp)__ seems reasonable.\n",
    "\n",
    " Reasonable, as for fine-tuning fueled by English tweets, conforming to scraped data. Those tweets featured date from not too long ago, supporting the predictions' temporal relevance with respect to language's ongoing evolution. Such predictions also adhere to the envisioned nature of a sentiment, showcasing promising quality as of ad-hoc testing. Ad-hoc tests are further validated by popularity indicative of quality.\n",
    "\n",
    " Such quality, however, requires preprocessing. __[Research](https://towardsdatascience.com/does-bert-need-clean-data-part-2-classification-d29adf9f745a)__ advocates light preprocessing, as BERT and derivatives, such as RoBERTa, heavily rely on contextual information. In accordance, emphasis shall be given to noise. Starting with padding, rooted in typographic norms, organizing devoid of semantics, yet lent by transformers. Emails, mentions, and websites abound with semantics in a similar fashion, raised, yet not necessarily intended for attribution in their nomenclature. Numerals, typically factual in nature, may also cause confusion, by virtue of their semantic neutrality as regards sentiment. While alphanumerics tend to lack linguistic coherence, fostering ambiguity and, thus, misinterpretation. All of that noise calls for mitigation.\n",
    "\n",
    " Diverse representations bring about quality risks, apart from noise. Semantically similar words may thus differ in meaning due to variations in their written forms. Here, emphasis is placed on prevailing standards—those variations, facing greatest exposure in pre-training. Given that greater exposure comes along with greater contextual awareness. As English texts constituted the predominant source, ASCII character encoding was enforced where feasible. Those English texts were mainly sourced from Brown's corpus and Wikipedia, whereas formal writing dominates, in favor of uncontracted forms. Words within such texts were lowercased, passed on to preprocessing.\n",
    "\n",
    " Following consideration of these aspects, corpora are then classified by sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "noun_rec: 100%|██████████| 13368/13368 [41:14<00:00,  5.40texts/s] \n",
      "verb_rec: 100%|██████████| 13368/13368 [41:00<00:00,  5.43texts/s] \n"
     ]
    }
   ],
   "source": [
    "#extract part-of-speech\n",
    "for lexeme in [\"NOUN\", \"VERB\"]:\n",
    "    nlp.lexemes(lexeme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among those sentiments, certain are more pertinent to the project than others. Emphasis lies therefore on sentiments expressed in tweets pertaining to education, a niche we are eager to explore. Topics such as education are in essence captured by nouns and verbs. Nouns, as designators of objects, and verbs, as expressions of action. Those parts-of-speech may be tagged via **_spaCy_**. As for the tweets' language, English **_spaCy_** models are considered. Of those __[\"en_core_web_trf\"](https://github.com/explosion/spacy-models/releases/tag/en_core_web_trf-3.7.3)__ stood out as the most accurate. Such accuracy calls for preprocessing.\n",
    "\n",
    "The primary focus of preprocessing lies in maximizing recognition of parts-of-speech in the **_spaCy_** model. Thus, gerundial forms are abrogated, giving rise to common standards. For all conjugations to be treated, apostrophes shall be encoded as ASCII characters. Reduplications of resulting regular forms, largely indistinguishable from consunant doubling are at least reduced to such. By doing so orthographically valid or at least close renditions are reached. Those renditions exclude entities such as emails, mentions, and websites so as to avoid the semantic mismatch previously mentioned.\n",
    "\n",
    "Recognized part-of-speech is then reduced by the utmost aggressive stemmer, \"Lancaster\". Nouns and verbs within a family are thereby brought to a common denominator (e.g. noun: education, verb: educate, stem: educ). Downstream matching is thus simplified, as both noun and verb share stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.df.to_csv(\"tweets_processed.csv\",\n",
    "              index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting data is then stored in a CSV file for reference. Stored as checkpoint in case of future analysis beyond this project's reach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#EB7101; font-family: arial; color: #ffffff; font-size: 200%; text-align: center; border-radius: 15px 15px;\">4 - Analysis</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural language processed checkpoint, constitutes the foundation for analysis. Such an analysis reveals insights rooted in that foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana = analysis(\"tweets_processed.csv\", [\"noun_stems\", \"verb_stems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "matching: 100%|██████████| 13368/13368 [00:01<00:00, 10830.99texts/s]\n"
     ]
    }
   ],
   "source": [
    "#filter for keywords\n",
    "ana.match(\"education\", [\"education\", \"expert\", \"learning\", \"teacher\", \"tutor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing the foundation into harmony with the interest niche constitutes the first step. The scope of analysis thus should be narrowed to education. Part-of-speech is therefore filtered, based on assumption-wise representative education stems. Representative stems are listed below.\n",
    "\n",
    "<center>\n",
    "\n",
    "|Vocabulary|Stem|\n",
    "|:----:|:----:|\n",
    "|education|educ|\n",
    "|expert|expert|\n",
    "|learning|learn|\n",
    "|teacher|teach|\n",
    "|tutor|tut|\n",
    "\n",
    "</center>\n",
    "\n",
    "Yet, as stemmers may not always perform as intended, encapsulations are matched via **_TheFuzz_**'s partial ratio. Such matching may, however, cause the inclusion of widespreaded short stems, rather than actual encapsulations. For this reason, stems of at least the same length of the shortest qualify as matches. Matches (1,172) are deemed educationally relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove spam and enable individual keyword tracking\n",
    "df_vis = (ana.df[ana.df[\"education_relevancy\"]]\n",
    "          .drop_duplicates(\n",
    "              subset = [\n",
    "                  \"user_identifier\", \n",
    "                  \"text\"\n",
    "              ])\n",
    "          .explode(\n",
    "              column = \"match\"\n",
    "          )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant tweets for education are screened for duplicates by author and text. Duplicates found are discarded, mitigating spamming-induced bias, leaving 1,013 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['educ$educ' 'learn$learn' 'tut$tut' 'expert$expert' 'learn$machinelearn'\n",
      " 'teach$teachersoftwit' 'learn$languagelearn' 'educ$reduc' 'teach$teach'\n",
      " 'learn$learnt' 'learn$englishlearn' 'learn$learningwitha' 'learn$earn'\n",
      " 'teach$tea' 'learn$deeplearn' 'learn$ear' 'expert$kqlexpertise'\n",
      " 'educ$techineduc' 'tut$techtut' 'tut$institut' 'learn$ailearn'\n",
      " 'teach$teachertwit' 'tut$substitut' 'educ$aieduc' 'educ$deduc' 'educ$edu'\n",
      " 'educ$earlyeduc' 'educ$duc' 'learn$learningmadeeasy'\n",
      " 'learn$lifelonglearn' 'learn$create2learn' 'tut$constitut' 'tut$futut'\n",
      " 'tut$photoshoptut' 'educ$educationinnov' 'learn$learningjourney'\n",
      " 'learn$learningwithlaugh' 'educ$cannabiseduc' 'learn$laughandlearn'\n",
      " 'educ$educationrevolv' 'learn$learning-' 'learn$-learning'\n",
      " 'learn$-learned' 'expert$seoexpert' 'learn$unlearn' 'learn$gamifiedlearn'\n",
      " 'teach$teachers&student' 'educ$highereduc']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    df_vis[\"match\"].unique()\n",
    ")\n",
    "irrelevant = [\n",
    "    \"machinelearn\",\n",
    "    \"reduc\",\n",
    "    \"earn\",\n",
    "    \"tea\",\n",
    "    \"deeplearn\",\n",
    "    \"ear\",\n",
    "    \"institut\",\n",
    "    \"teachertwit\",\n",
    "    \"substitut\",\n",
    "    \"deduc\",\n",
    "    \"duc\",\n",
    "    \"constitut\",\n",
    "    \"unlearn\",\n",
    "    \"teachers&student\"\n",
    "]\n",
    "\n",
    "#separate matching pairs\n",
    "df_vis[[\"match_with\", \"match_base\"]] = df_vis[\"match\"].str.split(\n",
    "    pat = \"$\", \n",
    "    n = 1, \n",
    "    expand = True\n",
    ")\n",
    "\n",
    "#remove erroneously matched keywords\n",
    "df_vis.drop(\n",
    "    df_vis[df_vis[\"match_base\"].isin(irrelevant)].index,\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining tweets face scrutiny given the ambiguity associated with filtered stems. Ambiguity may introduce semantic mismatches caused by **_TheFuzz_**. Stems deemed non-relevant to our niche are manually omitted, leaving 780 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-32b39ddec14846198c5f9ab95ee75970.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-32b39ddec14846198c5f9ab95ee75970.vega-embed details,\n",
       "  #altair-viz-32b39ddec14846198c5f9ab95ee75970.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-32b39ddec14846198c5f9ab95ee75970\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-32b39ddec14846198c5f9ab95ee75970\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-32b39ddec14846198c5f9ab95ee75970\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-39c232177b27b7d785488416dd57bce2\"}, \"mark\": {\"type\": \"bar\", \"size\": 49}, \"encoding\": {\"color\": {\"field\": \"sentiment\", \"legend\": {\"title\": \"Sentiment\"}, \"scale\": {\"range\": [\"#EB7101\", \"#DEDEDE\", \"#75A99C\"]}, \"type\": \"nominal\"}, \"order\": {\"field\": \"color_sentiment_index\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"count\", \"title\": \"Quantity\", \"type\": \"quantitative\"}, {\"field\": \"share\", \"format\": \".0%\", \"title\": \"Share\", \"type\": \"quantitative\"}], \"x\": {\"aggregate\": \"sum\", \"axis\": {\"labelExpr\": \"datum.value * 100\", \"title\": \"Public Perception in Percentage (%)\"}, \"field\": \"count\", \"stack\": \"normalize\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"title\": \"Stem\"}, \"field\": \"match_with\", \"type\": \"nominal\"}}, \"height\": 250, \"title\": \"Public Perception Distribution of Tweets on GPTs in Education\", \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-39c232177b27b7d785488416dd57bce2\": [{\"match_with\": \"educ\", \"sentiment\": \"Negative\", \"count\": 6, \"share\": 0.07317073170731707}, {\"match_with\": \"educ\", \"sentiment\": \"Neutral\", \"count\": 29, \"share\": 0.35365853658536583}, {\"match_with\": \"educ\", \"sentiment\": \"Positive\", \"count\": 47, \"share\": 0.573170731707317}, {\"match_with\": \"expert\", \"sentiment\": \"Negative\", \"count\": 6, \"share\": 0.03571428571428571}, {\"match_with\": \"expert\", \"sentiment\": \"Neutral\", \"count\": 52, \"share\": 0.30952380952380953}, {\"match_with\": \"expert\", \"sentiment\": \"Positive\", \"count\": 110, \"share\": 0.6547619047619048}, {\"match_with\": \"learn\", \"sentiment\": \"Negative\", \"count\": 24, \"share\": 0.05867970660146699}, {\"match_with\": \"learn\", \"sentiment\": \"Neutral\", \"count\": 146, \"share\": 0.3569682151589242}, {\"match_with\": \"learn\", \"sentiment\": \"Positive\", \"count\": 239, \"share\": 0.5843520782396088}, {\"match_with\": \"teach\", \"sentiment\": \"Negative\", \"count\": 4, \"share\": 0.04878048780487805}, {\"match_with\": \"teach\", \"sentiment\": \"Neutral\", \"count\": 34, \"share\": 0.4146341463414634}, {\"match_with\": \"teach\", \"sentiment\": \"Positive\", \"count\": 44, \"share\": 0.5365853658536586}, {\"match_with\": \"tut\", \"sentiment\": \"Negative\", \"count\": 6, \"share\": 0.05042016806722689}, {\"match_with\": \"tut\", \"sentiment\": \"Neutral\", \"count\": 46, \"share\": 0.3865546218487395}, {\"match_with\": \"tut\", \"sentiment\": \"Positive\", \"count\": 67, \"share\": 0.5630252100840336}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare data for share in tooltip\n",
    "alt_vis = (\n",
    "    df_vis.groupby(\n",
    "        [\n",
    "            \"match_with\", \n",
    "            \"sentiment\"\n",
    "        ]\n",
    "    )\n",
    "    .size()\n",
    "    .reset_index(\n",
    "        name = \"count\"\n",
    "    )\n",
    "    .assign(\n",
    "        share = lambda row: row[\"count\"] / row.groupby(\"match_with\")[\"count\"].transform(\"sum\"),\n",
    "        sentiment = lambda row: row[\"sentiment\"].str.title()\n",
    "    )\n",
    "    .sort_values(\n",
    "        [\n",
    "            \"match_with\", \n",
    "            \"sentiment\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "alt.Chart(alt_vis).mark_bar(size = 49).encode(\n",
    "    x = alt.X(\n",
    "        shorthand = \"sum(count):Q\",\n",
    "        stack = \"normalize\",\n",
    "        axis = alt.Axis(\n",
    "            title = \"Public Perception in Percentage (%)\",\n",
    "            labelExpr = \"datum.value * 100\",\n",
    "        )\n",
    "    ),\n",
    "    y = alt.Y(\n",
    "        shorthand = \"match_with:N\",\n",
    "        axis = alt.Axis(\n",
    "            title = \"Stem\",\n",
    "        ),\n",
    "    ),\n",
    "    color = alt.Color(\n",
    "        shorthand = \"sentiment:N\",\n",
    "        scale = alt.Scale(\n",
    "            range = [\n",
    "                \"#EB7101\", \n",
    "                \"#DEDEDE\",\n",
    "                \"#75A99C\"\n",
    "            ]\n",
    "        ),\n",
    "        legend = alt.Legend(\n",
    "            title = \"Sentiment\"\n",
    "        )\n",
    "    ),\n",
    "    order = alt.Order(\n",
    "        \"color_sentiment_index:Q\"\n",
    "    ),\n",
    "    tooltip = [\n",
    "        alt.Tooltip(\n",
    "            shorthand = \"count:Q\",\n",
    "            title = \"Quantity\"\n",
    "        ),\n",
    "        alt.Tooltip(\n",
    "            shorthand = \"share:Q\",\n",
    "            title = \"Share\",\n",
    "            format = \".0%\"\n",
    "        )\n",
    "    ]\n",
    ").properties(\n",
    "    title = \"Public Perception Distribution of Tweets on GPTs in Education\",\n",
    "    width = 500,\n",
    "    height = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the tweets are deemed representative of niche subject matter. Their sentiments are illustrated in a normalized stacked bar chart. Given varying population sizes per stem, prone to obscuration, distributions are investigated in solitude. Yet sentiment distributions exhibit clear structures across all stems. In fact, a modest majority gravitates towards GPTs. Slightly less than that are neutrally attuned. While poorly received by only a minority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#EB7101; font-family: arial; color: #ffffff; font-size: 200%; text-align: center; border-radius: 15px 15px;\">5 - Conclusion</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dialogue-optimized large language models have experienced explosive popularity of late. In the wake of that hype, research revealed dynamics, opposing traditional e-learning platforms. In keeping with this hype, GPTs were researched, regarded as more feasible alternative to in-house approaches. Research centered around GPTs education vertical in pursuit of investment indicators. The indicator explored in this project constitutes public perception, measured by sentiment.\n",
    "\n",
    "Sentiments were rooted in scraped tweets, classified by RoBERTa (fine-tuned). With relevancy anchored in stem-based filtering. Stems constituted part-of-speech tags pertaining to nouns and verbs. Resulting findings indicate a lucid majority. Public perception is thus far more positive than negative. GPTs exploratory investments are therefore supported by the indicator at first glance.\n",
    "\n",
    "This finding should, however, be treated with caution, as localized perceptions are evident. Localized by reference to a snapshot grounded in hypothesis-driven filtering, along with platform-specific scope. Within that snapshot, uncertainty persisted regarding perception emitters, as either end-users or vendors, indicative of the overall health of the ecosystem.\n",
    "\n",
    "As a result, there is ample room for future research. The scope may thus be broadened to include other platforms. While hypothesis-based filtering may be substituted for topic classification or modeling. Even so, further investigation into the type of user might be beneficial, gasping the ecosystem's health. Besides tweet emitting users, replies may also be sourced for weighting purposes.\n",
    "\n",
    "Nonetheless the project sheds light on an indicator, yet to be taken with a grain of salt. All the while setting the stage as cornerstone for future research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
